{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e3198e-733b-49c8-80c6-f3b9050749ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TestResults,TrainSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "import numpy as np\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a21448-a9f1-4586-8fb4-e948b08c5479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data\n",
      "Categorial Feature Imputed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NUM IMPUTE: 100%|█████████████████████████████████████| 11/11 [00:00<00:00, 56.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting the Data based on time\n",
      "X_train:(3387880, 80), X_test:(97972, 80) , y_train:(3387880, 2) , y_test:(97972, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = TrainSplit().get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c38bb9-48cc-4db8-bee1-5693792851fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = TrainSplit.CATEGORIES + TrainSplit.BINARY\n",
    "num_feat = TrainSplit.NUMERICAL\n",
    "all_feat = cat_feat + num_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202cc418-aa46-4f98-86b6-745dce66a9fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CategoricalNB in module sklearn.naive_bayes:\n",
      "\n",
      "class CategoricalNB(_BaseDiscreteNB)\n",
      " |  CategoricalNB(*, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
      " |  \n",
      " |  Naive Bayes classifier for categorical features\n",
      " |  \n",
      " |  The categorical Naive Bayes classifier is suitable for classification with\n",
      " |  discrete features that are categorically distributed. The categories of\n",
      " |  each feature are drawn from a categorical distribution.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <categorical_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, default=1.0\n",
      " |      Additive (Laplace/Lidstone) smoothing parameter\n",
      " |      (0 for no smoothing).\n",
      " |  \n",
      " |  fit_prior : bool, default=True\n",
      " |      Whether to learn class prior probabilities or not.\n",
      " |      If false, a uniform prior will be used.\n",
      " |  \n",
      " |  class_prior : array-like of shape (n_classes,), default=None\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  min_categories : int or array-like of shape (n_features,), default=None\n",
      " |      Minimum number of categories per feature.\n",
      " |  \n",
      " |      - integer: Sets the minimum number of categories per feature to\n",
      " |        `n_categories` for each features.\n",
      " |      - array-like: shape (n_features,) where `n_categories[i]` holds the\n",
      " |        minimum number of categories for the ith column of the input.\n",
      " |      - None (default): Determines the number of categories automatically\n",
      " |        from the training data.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  category_count_ : list of arrays of shape (n_features,)\n",
      " |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
      " |      for each feature. Each array provides the number of samples\n",
      " |      encountered for each class and category of the specific feature.\n",
      " |  \n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      Number of samples encountered for each class during fitting. This\n",
      " |      value is weighted by the sample weight when provided.\n",
      " |  \n",
      " |  class_log_prior_ : ndarray of shape (n_classes,)\n",
      " |      Smoothed empirical log probability for each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Class labels known to the classifier\n",
      " |  \n",
      " |  feature_log_prob_ : list of arrays of shape (n_features,)\n",
      " |      Holds arrays of shape (n_classes, n_categories of respective feature)\n",
      " |      for each feature. Each array provides the empirical log probability\n",
      " |      of categories given the respective feature and class, ``P(x_i|y)``.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      Number of features of each sample.\n",
      " |  \n",
      " |  n_categories_ : ndarray of shape (n_features,), dtype=np.int64\n",
      " |      Number of categories for each feature. This value is\n",
      " |      inferred from the data or set by the minimum number of categories.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> rng = np.random.RandomState(1)\n",
      " |  >>> X = rng.randint(5, size=(6, 100))\n",
      " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> from sklearn.naive_bayes import CategoricalNB\n",
      " |  >>> clf = CategoricalNB()\n",
      " |  >>> clf.fit(X, y)\n",
      " |  CategoricalNB()\n",
      " |  >>> print(clf.predict(X[2:3]))\n",
      " |  [3]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CategoricalNB\n",
      " |      _BaseDiscreteNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Naive Bayes classifier according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features. Here, each feature of X is\n",
      " |          assumed to be from a different categorical distribution.\n",
      " |          It is further assumed that all categories of each feature are\n",
      " |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
      " |          total number of categories for the given feature. This can, for\n",
      " |          instance, be achieved with the help of OrdinalEncoder.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance overhead hence it is better to call\n",
      " |      partial_fit on chunks of data that are as large as possible\n",
      " |      (as long as fitting in the memory budget) to hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features. Here, each feature of X is\n",
      " |          assumed to be from a different categorical distribution.\n",
      " |          It is further assumed that all categories of each feature are\n",
      " |          represented by the numbers 0, ..., n - 1, where n refers to the\n",
      " |          total number of categories for the given feature. This can, for\n",
      " |          instance, be achieved with the help of OrdinalEncoder.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseDiscreteNB:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CategoricalNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfab0ed-78b4-4f16-986c-2c9b87f764f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = CategoricalNB(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa551905-8960-4f3d-a3c1-2f1c4eddd739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalNB(alpha=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.fit(X_train[cat_feat],y_train[TrainSplit.IS_INSTALLED[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82744747-bdfb-4380-8603-d74a73f4f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv('../Data/test/000000000000.csv',sep='\\t')\n",
    "test['f_30'].fillna(test['f_30'].mode()[0],inplace=True)\n",
    "test['f_31'].fillna(test['f_31'].mode()[0],inplace=True)\n",
    "fmiss = \"f_43,f_51,f_58,f_59,f_64,f_65,f_66,f_67,f_68,f_69,f_70\".split(',')\n",
    "for f in fmiss:\n",
    "    test[f].fillna(test[f].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10607693-80a0-4e5a-afad-1d2968b75b58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_1\n",
      "{67}\n",
      "####################\n",
      "f_2\n",
      "{30041, 6035, 28863}\n",
      "####################\n",
      "f_4\n",
      "{24387, 24333, 14381, 24493, 6230}\n",
      "####################\n",
      "f_6\n",
      "{18433, 7554, 16001, 27784, 30344, 22664, 32267, 24456, 11920, 10385, 27410, 27923, 25364, 1176, 6169, 2843, 14370, 19363, 27428, 27045, 934, 26662, 11172, 11430, 20134, 29603, 25644, 13477, 13695, 32297, 4401, 9011, 437, 5046, 23609, 23228, 2496, 6723, 3652, 13124, 6597, 11983, 3408, 22863, 31440, 28116, 11481, 13274, 9052, 8542, 27231, 19679, 2150, 4072, 1001, 12394, 24811, 20075, 21613, 30959, 30065, 16370, 28660, 31989, 12025, 20858, 19327}\n",
      "####################\n",
      "f_13\n",
      "{19541, 29366}\n",
      "####################\n",
      "f_15\n",
      "{24835, 2563, 3588, 21127, 16135, 8841, 27530, 4363, 6666, 15116, 22286, 7183, 7434, 3474, 9493, 1563, 20128, 8609, 14753, 2084, 22183, 5800, 428, 28980, 4022, 13243, 16318, 19775, 22468, 20166, 30409, 20426, 5328, 14035, 5592, 29659, 7004, 31838, 20703, 13152, 20962, 17125, 17513, 9321, 28267, 4728, 12666, 6384, 17779, 20216, 3577, 16506, 26111}\n",
      "####################\n",
      "f_16\n",
      "{8448, 6270}\n",
      "####################\n",
      "f_18\n",
      "{31620, 5769, 2062, 10639, 13586, 20759, 24856, 22812, 5790, 23843, 31407, 4410, 19901, 17106, 29526, 32604, 863, 29030, 2792, 1131, 25963, 26603, 24444}\n",
      "####################\n",
      "f_20\n",
      "{30994, 29750}\n",
      "####################\n",
      "f_21\n",
      "{30591}\n",
      "####################\n",
      "f_22\n",
      "{9968, 13228}\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,42):\n",
    "    feat = f\"f_{i}\"\n",
    "    out = set(test[feat].unique()) -  set(X_test[feat].unique()).union(set(X_train[feat].unique()))\n",
    "    if len(out):\n",
    "        print(feat)\n",
    "        print(out)\n",
    "        print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b92ed62d-ab3b-407a-9c3a-cdc0479e34af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1074,\n",
       " 1140,\n",
       " 2121,\n",
       " 2166,\n",
       " 2388,\n",
       " 3570,\n",
       " 4306,\n",
       " 4972,\n",
       " 6458,\n",
       " 7227,\n",
       " 8310,\n",
       " 8327,\n",
       " 9584,\n",
       " 11708,\n",
       " 14830,\n",
       " 14913,\n",
       " 14994,\n",
       " 15937,\n",
       " 15957,\n",
       " 17019,\n",
       " 17637,\n",
       " 17736,\n",
       " 17914,\n",
       " 18391,\n",
       " 18575,\n",
       " 19571,\n",
       " 19746,\n",
       " 20372,\n",
       " 21231,\n",
       " 21538,\n",
       " 21713,\n",
       " 22007,\n",
       " 22228,\n",
       " 23666,\n",
       " 24394,\n",
       " 25354,\n",
       " 25560,\n",
       " 25600,\n",
       " 26828,\n",
       " 27168,\n",
       " 27238,\n",
       " 27812,\n",
       " 28048,\n",
       " 29670,\n",
       " 29943,\n",
       " 29958,\n",
       " 31058,\n",
       " 31785,\n",
       " 32028,\n",
       " 32477}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = f\"f_{15}\"\n",
    "set(X_test[feat].unique()) - set(X_train[feat].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdf3e50b-bfc7-4c2e-a95c-b6c3e93395a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X_train[feat].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc26cffb-fb82-4efc-9d8f-5b3638e78baf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 32765 is out of bounds for axis 1 with size 32763",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat_feat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9_global/lib/python3.9/site-packages/sklearn/naive_bayes.py:115\u001b[0m, in \u001b[0;36m_BaseNB.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Return probability estimates for the test vector X.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_log_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/venvs/python3.9_global/lib/python3.9/site-packages/sklearn/naive_bayes.py:95\u001b[0m, in \u001b[0;36m_BaseNB.predict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     93\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     94\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[0;32m---> 95\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_joint_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m log_prob_x \u001b[38;5;241m=\u001b[39m logsumexp(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/python3.9_global/lib/python3.9/site-packages/sklearn/naive_bayes.py:1303\u001b[0m, in \u001b[0;36mCategoricalNB._joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_):\n\u001b[1;32m   1302\u001b[0m     indices \u001b[38;5;241m=\u001b[39m X[:, i]\n\u001b[0;32m-> 1303\u001b[0m     jll \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_log_prob_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1304\u001b[0m total_ll \u001b[38;5;241m=\u001b[39m jll \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_log_prior_\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_ll\n",
      "\u001b[0;31mIndexError\u001b[0m: index 32765 is out of bounds for axis 1 with size 32763"
     ]
    }
   ],
   "source": [
    "out = nb_model.predict_proba(X_test[cat_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22104d31-c43a-4076-b1f6-6080a7959d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
